{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d805aa3",
   "metadata": {},
   "source": [
    "# Minimal RLOO Fine-Tuning for Code Tasks (MBPP → HumanEval)\n",
    "\n",
    "This notebook shows a **minimal end‑to‑end** workflow to fine‑tune a small code model with **RLOO** (Reinforcement Learning with Leave‑One‑Out) on **MBPP (sanitized)** and then **evaluate on HumanEval**.\n",
    "\n",
    "**Default model:** `HuggingFaceTB/SmolLM2-360M-Instruct` (tiny & fast).  \n",
    "**Swap-in option:** Any compatible HF model id (e.g., `Qwen/Qwen2.5-Coder-1.5B-Instruct`).\n",
    "\n",
    "> ⚠️ **Security note**: This workflow **executes model‑generated Python** when scoring with unit tests. Run in a controlled environment (e.g., Docker/VM). You can also use `evalplus` which isolates execution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a7cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.10.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.2.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: trl in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.24.0)\n",
      "Collecting trl\n",
      "  Downloading trl-0.25.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.17.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6.2)\n",
      "Requirement already satisfied: human-eval in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.3)\n",
      "Requirement already satisfied: evalplus in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.1)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.48.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: anyio in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0->datasets) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: fire in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from human-eval) (0.7.1)\n",
      "Requirement already satisfied: wget>=3.2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (3.2)\n",
      "Requirement already satisfied: tempdir>=0.7.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (0.7.1)\n",
      "Requirement already satisfied: multipledispatch>=0.6.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (1.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (1.4.4)\n",
      "Requirement already satisfied: termcolor>=2.0.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (2.4.0)\n",
      "Requirement already satisfied: openai>=1.11.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (1.107.0)\n",
      "Requirement already satisfied: tree-sitter>=0.22.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (0.25.2)\n",
      "Requirement already satisfied: tree-sitter-python>=0.21.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (0.25.0)\n",
      "Requirement already satisfied: rich>=12.3.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (13.7.1)\n",
      "Requirement already satisfied: stop-sequencer>=1.2.3 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (1.2.3)\n",
      "Requirement already satisfied: anthropic>=0.34.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (0.64.0)\n",
      "Requirement already satisfied: google-generativeai>=0.7.2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evalplus) (0.8.5)\n",
      "Requirement already satisfied: networkx in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anthropic>=0.34.1->evalplus) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anthropic>=0.34.1->evalplus) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anthropic>=0.34.1->evalplus) (2.12.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anthropic>=0.34.1->evalplus) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->anthropic>=0.34.1->evalplus) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->anthropic>=0.34.1->evalplus) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->anthropic>=0.34.1->evalplus) (0.4.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai>=0.7.2->evalplus) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai>=0.7.2->evalplus) (2.19.2)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai>=0.7.2->evalplus) (2.142.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai>=0.7.2->evalplus) (2.34.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai>=0.7.2->evalplus) (5.29.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.7.2->evalplus) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core->google-generativeai>=0.7.2->evalplus) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.7.2->evalplus) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.7.2->evalplus) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.7.2->evalplus) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.7.2->evalplus) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.7.2->evalplus) (4.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai>=0.7.2->evalplus) (0.6.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=12.3.0->evalplus) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=12.3.0->evalplus) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->evalplus) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai>=0.7.2->evalplus) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai>=0.7.2->evalplus) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai>=0.7.2->evalplus) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.7.2->evalplus) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jkorm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading trl-0.25.0-py3-none-any.whl (462 kB)\n",
      "Downloading bitsandbytes-0.48.2-py3-none-win_amd64.whl (59.0 MB)\n",
      "   ---------------------------------------- 0.0/59.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 3.1/59.0 MB 18.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 9.4/59.0 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 15.2/59.0 MB 24.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 21.0/59.0 MB 25.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 26.7/59.0 MB 25.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 32.2/59.0 MB 25.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 37.7/59.0 MB 26.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 43.3/59.0 MB 26.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 48.8/59.0 MB 26.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 53.5/59.0 MB 26.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  58.7/59.0 MB 25.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 59.0/59.0 MB 24.7 MB/s  0:00:02\n",
      "Installing collected packages: bitsandbytes, accelerate, datasets, trl\n",
      "\n",
      "  Attempting uninstall: bitsandbytes\n",
      "\n",
      "    Found existing installation: bitsandbytes 0.48.1\n",
      "\n",
      "    Uninstalling bitsandbytes-0.48.1:\n",
      "\n",
      "      Successfully uninstalled bitsandbytes-0.48.1\n",
      "\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "  Attempting uninstall: accelerate\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "    Found existing installation: accelerate 1.10.1\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "    Uninstalling accelerate-1.10.1:\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "      Successfully uninstalled accelerate-1.10.1\n",
      "   ---------------------------------------- 0/4 [bitsandbytes]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "  Attempting uninstall: datasets\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "    Found existing installation: datasets 4.2.0\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "    Uninstalling datasets-4.2.0:\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "      Successfully uninstalled datasets-4.2.0\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "  Attempting uninstall: trl\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "    Found existing installation: trl 0.24.0\n",
      "   -------------------- ------------------- 2/4 [datasets]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "    Uninstalling trl-0.24.0:\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "      Successfully uninstalled trl-0.24.0\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ---------------------------------------- 4/4 [trl]\n",
      "\n",
      "Successfully installed accelerate-1.11.0 bitsandbytes-0.48.2 datasets-4.4.1 trl-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# If running locally, uncomment the next line to ensure fresh packages.\n",
    "%pip install -U transformers accelerate datasets trl peft safetensors human-eval evalplus bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1158bf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping tensorflow as it is not installed.\n",
      "WARNING: Skipping tensorflow-intel as it is not installed.\n",
      "WARNING: Skipping tensorflow-io-gcs-filesystem as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "# Clean up TensorFlow packages to avoid import errors (not needed for this torch-only workflow)\n",
    "%pip uninstall -y tensorflow tensorflow-intel tensorflow-io-gcs-filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440e230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow spec: None\n",
      "HF_TOKEN present: True\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import os\n",
    "print(\"tensorflow spec:\", importlib.util.find_spec(\"tensorflow\"))\n",
    "print(\"HF_TOKEN present:\", bool(os.environ.get(\"HF_TOKEN\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c436c309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env keys starting with 'HF': ['HF_TOKEN']\n",
      "HF_TOKEN present: True\n",
      "HfFolder token present: True\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "hf_keys = [k for k in os.environ if k.startswith(\"HF\")]\n",
    "print(\"Env keys starting with 'HF':\", hf_keys)\n",
    "print(\"HF_TOKEN present:\", bool(os.environ.get(\"HF_TOKEN\")))\n",
    "print(\"HfFolder token present:\", bool(HfFolder.get_token()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "287cfcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_ready_mbpp_plus_path', 'completeness_check', 'get_dataset_metadata', 'get_mbpp', 'get_mbpp_plus', 'get_mbpp_plus_hash', 'make_cache', 'mbpp_deserialize_inputs', 'mbpp_serialize_inputs', 'stream_jsonl']\n"
     ]
    }
   ],
   "source": [
    "import evalplus.data.mbpp as mbpp_mod\n",
    "import inspect\n",
    "print([name for name, obj in inspect.getmembers(mbpp_mod) if inspect.isfunction(obj)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba376f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalplus size: 427\n",
      "Sample key: 2\n",
      "dict_keys(['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list'])\n"
     ]
    }
   ],
   "source": [
    "from evalplus.data.mbpp import get_mbpp\n",
    "import itertools\n",
    "mbpp_evalplus = get_mbpp()\n",
    "print(\"evalplus size:\", len(mbpp_evalplus))\n",
    "first_key = next(iter(mbpp_evalplus))\n",
    "print(\"Sample key:\", first_key)\n",
    "print(mbpp_evalplus[first_key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "166c10a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, math, textwrap, multiprocessing as mp, queue, signal, sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# Disable optional back ends we do not need (avoids pulling in TensorFlow/Flax)\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"USE_FLAX\"] = \"0\"\n",
    "os.environ.setdefault(\"HF_HUB_DISABLE_TELEMETRY\", \"1\")\n",
    "\n",
    "from transformers.utils import import_utils as _hf_import_utils\n",
    "_hf_import_utils.is_tf_available = lambda: False\n",
    "# also update the public shortcut to be safe\n",
    "from transformers import utils as _hf_utils\n",
    "_hf_utils.is_tf_available = lambda: False\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import RLOOConfig, RLOOTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# ---- Choose your base model\n",
    "MODEL_NAME = os.environ.get(\"BASE_MODEL\", \"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "# Examples:\n",
    "#   \"HuggingFaceTB/SmolLM2-360M-Instruct\"  (≈360M)\n",
    "#   \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"     (≈1.5B)\n",
    "#   \"Qwen/Qwen2.5-Coder-7B-Instruct\"       (heavy; consider 8-bit)\n",
    "\n",
    "# Generation defaults for RLOO and eval\n",
    "GEN_K = int(os.environ.get(\"RLOO_NUM_GENERATIONS\", 4))     # leave-one-out over K completions\n",
    "MAX_PROMPT_LEN = 512\n",
    "MAX_COMPLETION_LEN = 256\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06f6e5",
   "metadata": {},
   "source": [
    "## Load MBPP (sanitized) and build prompts\n",
    "We’ll use the **sanitized** split (427 items). It includes a problem prompt, canonical solution, and unit tests. For reliable function names, we **extract only the function signature** from the canonical solution and add it to the prompt (no leakage of solution body)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f04e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/sanitized-mbpp.json']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_repo_files\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "try:\n",
    "    # Explicitly mark this repo as a dataset; otherwise the hub looks under models and fails.\n",
    "    files = list_repo_files(\"Muennighoff/mbpp\", repo_type=\"dataset\", token=hf_token)\n",
    "    print([f for f in files if \"sanitized\" in f][:20])\n",
    "except RepositoryNotFoundError:\n",
    "    print(\"Cannot list 'Muennighoff/mbpp'. Accept the dataset terms and set HF_TOKEN first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7caea9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 4903.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data:\n",
      "\n",
      "Prompt text:\n",
      "\n",
      "You are a Python coding assistant.\n",
      "Write a **correct and efficient** solution that **defines exactly** this function signature and passes the hidden tests.\n",
      "\n",
      "# Problem\n",
      "Write a function to find the shared elements from the given two lists.\n",
      "\n",
      "# Function signature (must match exactly)\n",
      "def similar_elements(test_tup1, test_tup2):\n",
      "\n",
      "# Output format\n",
      "Return **only valid Python code** implementing the function. **No** markdown, comments, prints, or extra text.\n",
      "\n",
      "Test setup:\n",
      "\n",
      "\n",
      "\n",
      "Tests:\n",
      "\n",
      "['assert set(similar_elements((3, 4, 5, 6),(5, 7, 4, 10))) == set((4, 5))', 'assert set(similar_elements((1, 2, 3, 4),(5, 4, 3, 7))) == set((3, 4))', 'assert set(similar_elements((11, 12, 14, 13),(17, 15, 14, 13))) == set((13, 14))']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evalplus.data.mbpp import get_mbpp\n",
    "\n",
    "# Load sanitized MBPP via evalplus (avoids gated Hugging Face script)\n",
    "mbpp_dict = get_mbpp()\n",
    "mbpp = Dataset.from_list(list(mbpp_dict.values()))\n",
    "mbpp = mbpp.shuffle(seed=42)\n",
    "print(mbpp[0].keys())\n",
    "# expected fields: 'prompt', 'code', 'test_imports', 'test_list', 'challenge_test_list'\n",
    "\n",
    "# --- helpers to build a good training prompt ---\n",
    "DEF_RE = re.compile(r\"^\\s*def\\s+\\w+\\s*\\(.*\\)\\s*:\")\n",
    "\n",
    "def extract_signature(code: str) -> str:\n",
    "    for line in code.splitlines():\n",
    "        if DEF_RE.match(line):\n",
    "            return line.strip()\n",
    "    return \"def solution():\\n    pass\"\n",
    "\n",
    "\n",
    "def build_training_prompt(rec):\n",
    "    sig = extract_signature(rec[\"code\"])  # only the signature, no body\n",
    "    base = rec[\"prompt\"].strip()\n",
    "    prompt = f\"\"\"\n",
    "You are a Python coding assistant.\n",
    "Write a **correct and efficient** solution that **defines exactly** this function signature and passes the hidden tests.\n",
    "\n",
    "# Problem\n",
    "{base}\n",
    "\n",
    "# Function signature (must match exactly)\n",
    "{sig}\n",
    "\n",
    "# Output format\n",
    "Return **only valid Python code** implementing the function. **No** markdown, comments, prints, or extra text.\n",
    "\"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    "# Build a compact training set for a quick run\n",
    "train_size = int(os.environ.get(\"MBPP_TRAIN_ITEMS\", 150))  # try 150 for a few minutes; increase for better results\n",
    "subset = mbpp.select(range(train_size))\n",
    "\n",
    "train_records = subset.map(lambda r: {\n",
    "    \"prompt_text\": build_training_prompt(r),\n",
    "    \"test_setup\": \"\\n\".join(r.get(\"test_imports\", []) or []),\n",
    "    \"tests\": r.get(\"test_list\", []) or [],\n",
    "})\n",
    "\n",
    "print(\"Example data:\\n\")\n",
    "print(\"Prompt text:\\n\")\n",
    "print(train_records[0][\"prompt_text\"])\n",
    "print(\"\\nTest setup:\\n\")\n",
    "print(train_records[0][\"test_setup\"])\n",
    "print(\"\\nTests:\\n\")\n",
    "print(train_records[0][\"tests\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1fd1b",
   "metadata": {},
   "source": [
    "## Reward function\n",
    "The reward is **fraction of unit tests passed** (0.0–1.0) for each generated completion.  \n",
    "We execute code in a **separate process with a timeout** for basic isolation. For stricter isolation, consider running this notebook inside Docker and/or using `evalplus` for evaluation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48247e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example reward: [0.0]\n"
     ]
    }
   ],
   "source": [
    "def _mp_worker(q: mp.Queue, code: str, tests: list[str], setup: str):\n",
    "    \"\"\"Run student code plus tests in isolation and push (passed, total) to q.\"\"\"\n",
    "    try:\n",
    "        glb = {}\n",
    "        if setup:\n",
    "            exec(setup, glb, glb)\n",
    "        exec(code, glb, glb)\n",
    "        passed = 0\n",
    "        total = len(tests)\n",
    "        for t in tests:\n",
    "            try:\n",
    "                exec(t, glb, glb)\n",
    "                passed += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        q.put((passed, total))\n",
    "    except Exception:\n",
    "        q.put((0, len(tests)))\n",
    "\n",
    "\n",
    "def _exec_in_subproc(code: str, tests: list[str], setup: str, timeout_s: float = 3.0):\n",
    "    \"\"\"Execute in a short-lived process with a timeout; return (passed, total).\"\"\"\n",
    "    q = mp.Queue()\n",
    "    p = mp.Process(target=_mp_worker, args=(q, code, tests, setup))\n",
    "    p.start()\n",
    "    p.join(timeout_s)\n",
    "    if p.is_alive():\n",
    "        p.terminate()\n",
    "        return 0, len(tests)\n",
    "    try:\n",
    "        return q.get_nowait()\n",
    "    except queue.Empty:\n",
    "        return 0, len(tests)\n",
    "\n",
    "# Match fenced code blocks so we can pull out raw Python when the model returns markdown.\n",
    "CODE_FENCE_RE = re.compile(r\"```(?:python)?\\n(.*?)```\", re.DOTALL)\n",
    "\n",
    "def extract_code_only(txt: str) -> str:\n",
    "    \"\"\"Strip markdown fences or stray backticks; fall back to raw text.\"\"\"\n",
    "    m = CODE_FENCE_RE.search(txt)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return txt.strip().strip(\"`\")\n",
    "\n",
    "# TRL reward function signature: it receives completions alongside dataset columns.\n",
    "\n",
    "def mbpp_reward(completions, tests, test_setup, **kwargs):\n",
    "    \"\"\"Score each completion by fraction of MBPP tests that pass.\"\"\"\n",
    "    outs = []\n",
    "    for comp, ts, setup in zip(completions, tests, test_setup):\n",
    "        # TRL may give chat-format responses (list of role/content dicts); normalize to plain text.\n",
    "        if isinstance(comp, list) and len(comp) and isinstance(comp[0], dict) and \"content\" in comp[0]:\n",
    "            comp_text = comp[0][\"content\"]\n",
    "        else:\n",
    "            comp_text = str(comp)\n",
    "        code = extract_code_only(comp_text)\n",
    "        passed, total = _exec_in_subproc(code, ts, setup, timeout_s=3.0)\n",
    "        outs.append(0.0 if total == 0 else float(passed) / float(total))\n",
    "    return outs\n",
    "\n",
    "# quick self-check on one item\n",
    "_test_code = \"\"\"\n",
    "\"\"\"\n",
    "sc = train_records[0]\n",
    "_score = mbpp_reward([\n",
    "    \"\"\"def noop():\\n    return 0\"\"\"\n",
    "], tests=[sc[\"tests\"]], test_setup=[sc[\"test_setup\"]])\n",
    "print(\"example reward:\", _score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62558b",
   "metadata": {},
   "source": [
    "## Load tokenizer/model & configure RLOO (LoRA)\n",
    "We keep it small and cheap: LoRA adapters on a tiny model. Increase steps / dataset size later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf695d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkorm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\trl\\trainer\\rloo_trainer.py:241: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<trl.trainer.rloo_trainer.RLOOTrainer at 0x12708bca890>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer drives both prompt encoding and generation decoding during training/eval.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LoRA adapter hyperparameters control how much of the attention/MLP blocks we fine-tune.\n",
    "peft_cfg = LoraConfig(\n",
    "    r=8,  # rank of the low-rank adapters; higher values = more trainable params\n",
    "    lora_alpha=16,  # scales the adapter update; affects learning capacity\n",
    "    lora_dropout=0.05,  # regularizes adapter activations to curb overfitting\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "\n",
    "# Core RLOO loop settings determine optimization stability, sampling cost, and logging cadence.\n",
    "rloo_cfg = RLOOConfig(\n",
    "    output_dir=\"./smollm2-mbpp-rloo\",  # checkpoint + log destination reused by later save/load\n",
    "    learning_rate=2e-5,  # step size for adapter weights; too high destabilizes RLOO updates\n",
    "    per_device_train_batch_size=2,  # number of prompts per device; limited by VRAM during sampling\n",
    "    gradient_accumulation_steps=1,  # scales effective batch; increase to smooth rewards if memory allows\n",
    "    num_generations=GEN_K,  # completions per prompt; defines leave-one-out pool and runtime cost\n",
    "    generation_batch_size=GEN_K,  # how many generations sampled concurrently; impacts GPU RAM footprint\n",
    "    max_prompt_length=MAX_PROMPT_LEN,  # truncation guard so prompts fit into context window\n",
    "    max_completion_length=MAX_COMPLETION_LEN,  # cap on generated tokens; affects sampling time/test cost\n",
    "    beta=0.01,  # leave-one-out baseline strength; tunes variance reduction in RLOO objective\n",
    "    logging_steps=5,  # tensorboard/console logging frequency\n",
    "    save_steps=50,  # checkpoint interval that ties to checkpoints reused for eval\n",
    "    max_steps=int(os.environ.get(\"RLOO_MAX_STEPS\", 60)),  # total optimizer steps; governs training duration\n",
    ")\n",
    "\n",
    "# Trainer wires model loading, reward loop, dataset, and adapter config together.\n",
    "trainer = RLOOTrainer(\n",
    "    model=MODEL_NAME,  # base HF model id; governs architecture capacity and tokenizer compatibility\n",
    "    args=rloo_cfg,\n",
    "    reward_funcs=mbpp_reward,  # custom reward defined above; shapes gradient signal\n",
    "    processing_class=tokenizer,  # ensures prompts/generations reuse the tokenizer configured here\n",
    "    train_dataset=train_records,  # preprocessed MBPP subset\n",
    "    peft_config=peft_cfg,  # attaches LoRA adapters with the settings defined above\n",
    ")\n",
    "trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd3c63",
   "metadata": {},
   "source": [
    "### (Optional) Sanity check: generate before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67e50cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prompt (truncated) ---\n",
      "You are a Python coding assistant.\n",
      "Write a **correct and efficient** solution that **defines exactly** this function signature and passes the hidden tests.\n",
      "\n",
      "# Problem\n",
      "Write a function to find the shared elements from the given two lists.\n",
      "\n",
      "# Function signature (must match exactly)\n",
      "def similar_elements(test_tup1, test_tup2):\n",
      "\n",
      "--- Sampled completion ---\n",
      "# Example\n",
      "Test cases:\n",
      "[1, 2, 3, 4, 5], [2, 4, 6, 8]\n",
      "\n",
      "**Do not** include any tests.\n",
      "\n",
      "# Test cases\n",
      "[1, 2, 3, 4, 5], [3, 5, 7, 9]\n",
      "\n",
      "[1, 2, 3, 4, 5], [1, 4, 6, 7, 8]\n",
      "\n",
      "[1, 2, 3, 4, 5], [1, 2, 4, 5, 6]\n",
      "\n",
      "[1, 2, 3, 4, 5], [1, 2, 6, 8, 10]\n",
      "\n",
      "[1, 2, 3, 4, 5\n"
     ]
    }
   ],
   "source": [
    "prompt0 = train_records[0][\"prompt_text\"]\n",
    "inputs = tokenizer(prompt0, return_tensors=\"pt\").to(trainer.accelerator.device)\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "with torch.no_grad():\n",
    "    output_ids = trainer.model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=192,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "completion_ids = output_ids[0][input_len:]\n",
    "completion = tokenizer.decode(completion_ids, skip_special_tokens=True).strip()\n",
    "print(\"--- Prompt (truncated) ---\")\n",
    "print(\"\\n\".join(prompt0.splitlines()[:8]))\n",
    "print(\"\\n--- Sampled completion ---\")\n",
    "print(completion if completion else \"[empty completion]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9dd89",
   "metadata": {},
   "source": [
    "## Train (tiny demo run)\n",
    "Crank up `RLOO_MAX_STEPS` and `MBPP_TRAIN_ITEMS` for real gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f7781e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkorm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 15:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkorm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=0.0, metrics={'train_runtime': 940.3565, 'train_samples_per_second': 0.128, 'train_steps_per_second': 0.064, 'total_flos': 0.0, 'train_loss': 0.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88e9051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ./smollm2-mbpp-rloo\n"
     ]
    }
   ],
   "source": [
    "save_dir = rloo_cfg.output_dir\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b3216",
   "metadata": {},
   "source": [
    "## Evaluate on HumanEval (pass@1)\n",
    "We’ll create a `samples.jsonl` with one completion per task, then run the official HumanEval harness.\n",
    "\n",
    "> On Windows, prefer `pip install human-eval-windows`.\n",
    "> If you want more rigorous evaluation, see `evalplus` too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkorm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.2)\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.2)\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m humaneval \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/openai_humaneval\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Reuse trainer.model/tokenizer in plain HF generate for speed\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_completion\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m192\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Use plain text prompting (no chat template) to keep it simple\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "humaneval = load_dataset(\"openai/openai_humaneval\", split=\"test\")\n",
    "\n",
    "# Reuse trainer.model/tokenizer in plain HF generate for speed\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "\n",
    "def make_completion(prompt: str, max_new_tokens=192, temperature=0.2):\n",
    "    # Use plain text prompting (no chat template) to keep it simple\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                 temperature=temperature, eos_token_id=tokenizer.eos_token_id)\n",
    "    out = tokenizer.decode(gen_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return out.strip()\n",
    "\n",
    "# Build HumanEval prompts: use the provided \"prompt\" field directly\n",
    "samples_path = Path(\"./humaneval_samples.jsonl\")\n",
    "with samples_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in tqdm(humaneval):\n",
    "        prompt = row[\"prompt\"]\n",
    "        comp = make_completion(prompt)\n",
    "        # strip code fences if present\n",
    "        comp = re.sub(r\"^```(?:python)?|```$\", \"\", comp).strip()\n",
    "        rec = {\"task_id\": row[\"task_id\"], \"completion\": comp}\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(\"Wrote:\", samples_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5d96c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 621.61it/s]\n",
      "164it [00:00, 621.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:13<00:00, 11.84it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to humaneval_samples.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 15801.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass@1': np.float64(0.0)}\n",
      "pass@1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# If you hit issues on Windows, try: %pip install -U human-eval-windows && import human_eval\n",
    "from human_eval.evaluation import evaluate_functional_correctness\n",
    "\n",
    "results = evaluate_functional_correctness(str(samples_path))\n",
    "print(results)\n",
    "print(\"pass@1:\", results.get(\"pass@1\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae383bb",
   "metadata": {},
   "source": [
    "### Switching to Qwen2.5‑Coder\n",
    "To try Qwen:\n",
    "1. Change `MODEL_NAME` at the top to `Qwen/Qwen2.5-Coder-1.5B-Instruct`.\n",
    "2. (If VRAM-limited) add `load_in_8bit=True` to the model load by exporting `BITSANDBYTES_NOWELCOME=1` and using `bnb` (already included above).\n",
    "3. Increase `MAX_STEPS` and dataset size for meaningful gains.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
